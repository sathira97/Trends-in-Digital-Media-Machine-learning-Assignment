{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-PKyimL385f"
      },
      "source": [
        "import keras\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam, Nadam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Loading Dataset\n",
        "nb_classes         = 7\n",
        "img_rows, img_cols = 48, 48\n",
        "batch_size         = 32\n",
        "\n",
        "train_data_dir   = '/content/drive/MyDrive/MLAssignment/DataSet/train'\n",
        "test_data_dir    = '/content/drive/MyDrive/MLAssignment/DataSet/test'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "  train_data_dir,\n",
        "  color_mode  = 'grayscale',\n",
        "  target_size = (img_rows, img_cols),\n",
        "  batch_size  = batch_size,\n",
        "  class_mode  = 'categorical',\n",
        "  shuffle     = True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "\ttest_data_dir,\n",
        "  color_mode  = 'grayscale',\n",
        "  target_size = (img_rows, img_cols),\n",
        "  batch_size  = batch_size,\n",
        "  class_mode  = 'categorical',\n",
        "  shuffle     = True\n",
        ")\n",
        "\n",
        "\n",
        "#Loading data analysis\n",
        "train_set.class_indices\n",
        "\n",
        "def count_exp(path, set_):\n",
        "    dict_ = {}\n",
        "    for expression in os.listdir(path):\n",
        "        dir_ = path + expression\n",
        "        dict_[expression] = len(os.listdir(dir_))\n",
        "    df = pd.DataFrame(dict_, index=[set_])\n",
        "    return df\n",
        "train_count = count_exp(train_dir, 'train')\n",
        "test_count = count_exp(test_dir, 'test')\n",
        "print(train_count)\n",
        "print(test_count)\n",
        "\n",
        "# Plotting training data dist\n",
        "train_count.transpose().plot(kind = 'bar')\n",
        "\n",
        "# Plotting testing data dist\n",
        "test_count.transpose().plot(kind = 'bar')\n",
        "\n",
        "def plot_imgs(item_dir, top = 10):\n",
        "    all_item_dirs = os.listdir(item_dir)\n",
        "    item_files    = [os.path.join(item_dir, file) for file in all_item_dirs][:5]\n",
        "  \n",
        "    plt.figure(figsize = (10, 10))\n",
        "  \n",
        "    for idx, img_path in enumerate(item_files):\n",
        "        plt.subplot(5, 5, idx + 1)\n",
        "    \n",
        "        img = plt.imread(img_path)\n",
        "        plt.tight_layout()         \n",
        "        plt.imshow(img, cmap = 'gray')\n",
        "\n",
        "\n",
        "\n",
        "plot_imgs(train_data_dir + 'happy')\n",
        "\n",
        "plot_imgs(train_data_dir + 'angry')\n",
        "\n",
        "plot_imgs(train_data_dir + 'sad')\n",
        "\n",
        "\n",
        "#Building CNN model\n",
        "def build_model(nb_classes, input_shape):\n",
        "\n",
        "  model= tf.keras.models.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
        "  model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "    \n",
        "  model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512,(5,5), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  #Faltten the model\n",
        "  model.add(Flatten())\n",
        "    \n",
        "  model.add(Dense(256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.25))\n",
        "    \n",
        "  model.add(Dense(512))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = Adam(lr=0.0001 , decay=1e-6), \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Creating an instance of the model and printing the summary\n",
        "model = build_model(nb_classes, (img_rows, img_cols, 1))\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "plot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlRJuwbO7OJE"
      },
      "source": [
        "chk_path = 'modele1.h5'\n",
        "log_dir = \"/content/drive/MyDrive/MLAssignment/Logs\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath       = chk_path,\n",
        "    save_best_only = True,\n",
        "    verbose        = 1,\n",
        "    mode           = 'min',\n",
        "    moniter        = 'val_loss'\n",
        ")\n",
        "\n",
        "earlystop = EarlyStopping(\n",
        "    monitor              = 'val_loss', \n",
        "    min_delta            = 0, \n",
        "    patience             = 3, \n",
        "    verbose              = 1, \n",
        "    restore_best_weights = True\n",
        ")\n",
        "                        \n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor   = 'val_loss', \n",
        "    factor    = 0.2, \n",
        "    patience  = 6, \n",
        "    verbose   = 1, \n",
        "    min_delta = 0.0001\n",
        ")\n",
        "\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "csv_logger = CSVLogger('training.log')\n",
        "\n",
        "callbacks = [checkpoint, reduce_lr, csv_logger]\n",
        "\n",
        "def train_model(train, test, epochs, callbacks):\n",
        "  steps_per_epoch  = train.n // train.batch_size\n",
        "  validation_steps = test.n // test.batch_size\n",
        "\n",
        "  hist = model.fit(\n",
        "      x                = train, \n",
        "      validation_data  = test, \n",
        "      epochs           = epochs, \n",
        "      callbacks        = callbacks, \n",
        "      steps_per_epoch  = steps_per_epoch, \n",
        "      validation_steps = validation_steps\n",
        "  )\n",
        "\n",
        "  return hist\n",
        "\n",
        "\n",
        "  hist = train_model(train_set, test_set, 60, callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4tZHwSx70UJ"
      },
      "source": [
        "# Plotting the loss & accuracy\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kilHfPEG75ti"
      },
      "source": [
        "#Evaluvating \n",
        "train_loss, train_acc = model.evaluate(train_set)\n",
        "test_loss, test_acc   = model.evaluate(test_set)\n",
        "print(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_acc*100, test_acc*100))\n",
        "\n",
        "# Save the weights\n",
        "model.save_weights('fer2013_weights.h5')\n",
        "\n",
        "y_pred = model.predict(train_set)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "class_labels = test_set.class_indices\n",
        "class_labels = {v:k for k,v in class_labels.items()}\n",
        "\n",
        "cm_train = confusion_matrix(train_set.classes, y_pred)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(cm_train)\n",
        "\n",
        "print('Classification Report')\n",
        "target_names = list(class_labels.values())\n",
        "print(classification_report(train_set.classes, y_pred, target_names=target_names))\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cm_train, interpolation='nearest')\n",
        "plt.colorbar()\n",
        "tick_mark = np.arange(len(target_names))\n",
        "_ = plt.xticks(tick_mark, target_names, rotation=90)\n",
        "_ = plt.yticks(tick_mark, target_names)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}